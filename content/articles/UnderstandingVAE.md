---
title: Understanding Variational Auto-Encoders (VAEs)
description: Mapping variables into a compressed latent space
img: CelebFaces.gif
alt: Daily Routine Pic
author:
  name: Douglas
  bio: Douglas is a Software Engineer who is interested in computer vision and our quest for strong AI. He also is constantly looking for ways to push the envelope of his personal mental and physical fitness.
  image: ProfileDoug.jpg
  alt: Doug's profile pic
---

Recently, I have been reading through a variety of papers on world modeling. The idea being, that if you can have a compressed model of the world you can train on that simpler compressed model rather than working with the real data which has a lot of noise in it. On top of that, it should be possible to map the causal variables involved to one another to get a very extendable model for a machine to understand the problem space it was operating in. One promising way to compress the world space into a latent space of relevant features is with a VAE.

A VAE is a method of compressing data to some latent space, disentangling those variables from eachother so they are all independent and then using that compressed latent space to attempt to regenerate the original data with no knowledge of the original data other than the compressed latent space. 

The classic toy example of this is using convolutional neural networks on the MNIST dataset. I thought this was a little bit boring, so instead I worked through the example [here.](https://debuggercafe.com/generating-fictional-celebrity-faces-using-convolutional-variational-autoencoder-and-pytorch/) In it, instead of mapping numbers to a latent space, celebrity faces are mapped. Then new faces, albeit very low resolution faces in this case, can be generated by selecting a point in that latent space and passing those parameters to the decoder network.

<p align="center">
<v-image src="/CelebFaces.gif" alt="Celebrity Face convergance"></v-image>
<br/>
Celebrity Faces being generated over 75 epochs with the VAE
</p>

I then did the same thing with dog and cat pictures from the classic cat and dog classification dataset from kaggle.

<p align="center">
<v-image src="/gen_catsNDogs.gif" alt="Cats and Dogs convergance"></v-image>
<br/>
Cats and dogs attempting (and failing) to converge over 75 epochs
</p>

The cats and dogs attempt seemed to cause the network to really struggle so I tried again with only cat pictures. I think not having the clear definition of a face, as you do in the headshots of celebrity faces made the network really struggle to perform.

<p align="center">
<v-image src="/gen_cats75Epochs.gif" alt="Cats only convergance"></v-image>
<br/>
Cats only attempting to converge in 75 epochs
</p>

The cats only attempt also really struggled to perform. I think having irregular images that were always coerced to 64 x 64 caused severe warping to the input data. I also think that with this hard of a problem with this simple of a network, it just failed to generate the adorable fuzzballs I was expecting. That's alright though, it piques my curiosity on how to get decent performance on a non-trivial dataset or how to get higher resolution.

I did do an attempt with 200 epochs instead of 75, but because the network had largely converged at 75 epochs it largely was more of the same.

<p align="center">
    <v-image src="/gen_cats200Epochs.gif" alt="Cats only convergance 200 epochs" style="width: 30vw"></v-image>
    <v-image src="/gen_cats200EpochLoss.jpg" alt="Cats only convergance 200 epoch loss" style="width: 30vw"></v-image>
<br/>
Cats only attempting to converge in 200 epochs
</p>


# Next Steps

1. How do we get to have labeled variables in the latent space that are human readable? Is this a GAN only feature?
    1. Requires solid labeling of the data. In theory, you could define a latent space in the same way that you would with a GAN, you would just need to label your data 
    1. https://heartbeat.fritz.ai/stylegans-use-machine-learning-to-generate-and-customize-realistic-images-c943388dc672
        1. This is really interesting and shows what you can do with GAN's but doesn't answer my underlying question of how do you generate feature specific vectors in the latent space?
    1. This is touched on briefly in Deep Learning with Python by Francois Chollet, but it just suggests you have to search for vectors that might have the concept you are looking for.
    1. [Here](https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/) is a good breakdown of how to do vector arithmetic with GAN's. The same logic could be applied to VAEs. Display a collection of images organized by their positions in latent space. You can interpolate between them and another image by moving the generated images latent space points between the two points. With a large collection of images you can then generate a person with specific characteristics. Add or subtract values in latent space to get an image with the characteristics you want. This makes me a little sad, because this appears to be subtracting all of the values in the images latent space from one another. This suggests that we haven't really found a good concrete way to pull out a specific variable as of yet. (I could be wrong of course)
1. How do we apply a VAE to text?
    1. https://arxiv.org/pdf/1703.00955.pdf
1. What are some interesting models we could combine a VAE with?
    1. This will be ongoing.
1. Re-read the world-models papers to see if they are as interesting or as far reaching as I had initially thought now that I have a better understanding of what is going on under the hood.
    1. https://worldmodels.github.io/
        1. This is still excellent. The reason for that is the fact that it focuses on attempting to train a model rapidly on a simplified world model. I think this idea will be essential to modern explainable AI. I think that fairly simple idea is what is important. Not necessarily the fact that they are using a VAE on to simplify the images in particular. I think the next step would be to attempt to build out causality based world models that continue to be far simpler than the raw data most neural networks are trained on.
1. Read through the causal model with VAE papers I had found to see if they do what I think they are going to do in preventing overcontrolling for potentially related variables.
    1. https://arxiv.org/abs/2004.08697
        1. This is pretty good. It shows increased performance and understandability of the latent variables learned. It also builds out the causal models automatically from the brief skim that I gave it.
        1. I'm still not totally sure if a normal VAE would overcontrol it's variables in an attempt to get all indipendent variables though.
1. Learn how to build a VAE that can handle larger, higher resolution, non-square images.
    1. I'm pretty sure this isn't VAE specific... Just larger models, more compute, better pre-processing and cleaning of data.

## Final Thoughts

I need to spend more time building models compared to reading about them. Reading the math and trying to understand papers is essential, but I found a lot of concepts clicked when reading the code here. Most notably, how the loss functions worked. I think in many cases I try to understand complicated mathematical concepts for a particular machine learning model without working through a practical example. Working through this example gave me a solid understanding of how KL-divergence and cross-entropy work to give us a usable loss for a VAE.

This was a blast. I would love to be able to spend more than one day a week doing this.

If you are curious you can check out my slight variant on the original code [here](https://github.com/dschonholtz/FaceVAE).

Gotta keep chasing the excitement.

