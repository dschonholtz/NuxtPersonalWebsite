---
title: May 27th, 2021
description: May 27th
img: May.jpg
alt: May 2021 Calendar
author:
  name: Douglas
  bio: Douglas is a Software Engineer who is interested in computer vision and our quest for strong AI. He also is constantly looking for ways to push the envelope of his personal mental and physical fitness.
  image: ProfileDoug.jpg
  alt: Doug's profile pic
---

In my last post I said I would do the following the next day:
* Mantra
* 7:30 am workout.
* Cold shower
* work
* Follow up with some folks about apartments
* Podcast work
* The following day I should start the lstm reseach project

I actually did all of those except the workout. I was feeling burnt out for some reason.

I am torn between trying to do the arxiv project which is probably doable in a weekend or really just going all in on a research project.

I think the research project is the right way to go. I've found some really interesting stuff.

The very rough unedited thoughts are below:


Just read this: https://arxiv.org/pdf/2102.00572v1.pdf
Interpretable Reinforcement Learning Inspired by Piaget’s Theory of Cognitive Development 

There are a lot of ideas in my head I want to start looking at and combining.

I want to take ideas from cognitive development and apply them to machine intelligence.

How do you get a machine to have a conception of object permanence? 

How can you get a machine to have an underlying model of the world and then another model imagining what could change in that model and how they could impact it then having the machine make the corresponding actions required in the world to have the corresponding impact they imagined in their model?

The model controller architecture is explored in world models:
https://worldmodels.github.io/

And the thoughts there are loosely based off of Judea Pearl’s book The Book of Why: The new science of cause and effect. 

His work on his do calculus is fantastic but I’m not totally sure it is necessary for a machine to be doing causal analysis in the true sense of the word. I think it might be sufficient to have a model of the world that is significantly more compressed than the raw data the machine takes in that also is manipulatable and can interact with other models the machine might have. That would allow a machine to theoretically interact with those objects and climb the causal ladder as Judea pearl describes it by imagining what ifs and counterfactuals with those manipulatable world models.

This dovetails nicely with the thoughts in Interpretable Reinforcement Learning Inspired by Piaget’s Theory of Cognitive Development. There they attempt to define a model that learns by building and refining a world state and then acting on that world state as the model progresses. In the world models paper, the researchers show it is possible to have a model learn a compressed world model that can predict the next timestep, therefore, given an initial frame it can continuously do that and train inside of ideas of its own creation. 

This is exactly the world model we are looking for. 

It would be nice to combine the piaget inspired work with the world model inspired work.

So therefore, the world models would be mapped to schema based objects so they are more refined and more transportable to new situations and the encoder doesn’t overfit to the current situation.

Then the final thing after all of that that I would really like to see is another network that does oversight, it tries to correct the underlying algo by identifying problems in each of the underlying schemas. Then with that info fed to it the algo tries to adjust with that feedback.

Then as a final layer you would want a natural language description of that last layer that could describe to itself and others what it is correcting for and thinking about.
The idea being that the last two layers there would act something like our top layer of awareness or the conscious mind in Freud's model.

